# CodeTeacher

## ğŸš€ Usage Guide

### 1. Installation & Setup

```bash
# Install required packages
pip install gradio transformers torch accelerate bitsandbytes

# Run the application
python KullaniciArayuzu.ipynb
```

Or use **Google Colab** for GPU access (recommended).

### 2. Getting Started

#### Step 1: Select a Model
Choose from 6 available models in the dropdown:

**Toprak Henaz Models:**
- DeepSeek Coder 6.7B (ToprakH) - Best for code generation
- CodeLlama Magicoder 7B (ToprakH) - Great for code completion
- Gemma 7B (ToprakH) - General purpose model
- Qwen2.5 Coder 7B (ToprakH) - Advanced code analysis

**ByGedik Models:**
- Llama 3.1 8B CodeAlpaca (ByGedik) - Excellent code explanations
- Mistral CodeAlpaca V3 (ByGedik) - Quality code examples

#### Step 2: Load the Model
1. Click **"ğŸ“¥ Model YÃ¼kle"** to load your selected model
2. Wait for confirmation message: "âœ… Model baÅŸarÄ±yla yÃ¼klendi!"
3. The model is now cached and ready to use

#### Step 3: Enter Your Prompt
Write your coding question or request in the prompt box. Examples:
```
Python'da bir fibonacci fonksiyonu yaz ve aÃ§Ä±kla
JavaScript ile REST API Ã§aÄŸrÄ±sÄ± nasÄ±l yapÄ±lÄ±r?
SQL ile JOIN iÅŸlemleri Ã¶rneÄŸi
React functional component ile state management
```

#### Step 4: Adjust Parameters (Optional)
- **ğŸ“ Maksimum Uzunluk**: 50-1024 tokens (default: 512)
- **ğŸŒ¡ï¸ Temperature**: 0.1-2.0 (default: 0.7)
  - Lower = more consistent, Higher = more creative
- **ğŸ¯ Top-p**: 0.1-1.0 (default: 0.9)
  - Controls word choice diversity

#### Step 5: Generate Response
Click **"ğŸš€ Ãœret"** and get your AI-generated response!

### 3. Advanced Features

#### ğŸ”„ Model Switching
- **Smart Cache**: Up to 3 models cached simultaneously
- **Instant Switch**: Change between cached models in <1 second
- **Memory Management**: Automatic cleanup of oldest models

#### ğŸ“¦ Cache Management
- **"ğŸ“¦ Cache Durumu"**: Check current cached models
- **"ğŸ—‘ï¸ Bu Modeli Sil"**: Remove specific model from cache
- **"ğŸ§¹ TÃ¼mÃ¼nÃ¼ Temizle"**: Clear all cached models

#### ğŸ¯ Model Comparison
Test the same prompt across different models to compare:
1. Load Model A, enter prompt, generate
2. Switch to Model B (cached), same prompt, generate
3. Compare results instantly!

### 4. Usage Tips

#### ğŸ’¡ Best Practices
- **Start with DeepSeek-Coder** for general code generation
- **Use CodeLlama** when you need creative/diverse solutions
- **Try Qwen2.5-Coder** for complex code analysis
- **Cache 2-3 models** you use frequently
- **Adjust temperature** based on task:
  - Code generation: 0.3-0.7
  - Creative writing: 0.7-1.2
  - Problem solving: 0.5-0.9

#### ğŸš€ Performance Optimization
- **GPU Recommended**: Much faster generation
- **4-bit Quantization**: Automatically applied for memory efficiency
- **Batch Processing**: Load multiple models for comparison
- **Memory Monitoring**: Use cache status to manage resources

#### ğŸ¯ Prompt Engineering
**Good Prompts:**
```
âœ… "Python'da binary search algoritmasÄ± yaz ve her adÄ±mÄ± aÃ§Ä±kla"
âœ… "React'te useState hook'u nasÄ±l kullanÄ±lÄ±r? Ã–rnek ver"
âœ… "SQL'de INNER JOIN ve LEFT JOIN arasÄ±ndaki fark nedir?"
```

**Avoid:**
```
âŒ "Kod yaz" (too vague)
âŒ "Program yap" (unclear requirements)
âŒ "Help me" (no specific context)
```

### 5. Example Workflow

```python
# 1. Select Model
Model: "DeepSeek Coder 6.7B (ToprakH)"

# 2. Load Model
Click "ğŸ“¥ Model YÃ¼kle"
Status: "âœ… DeepSeek Coder 6.7B baÅŸarÄ±yla yÃ¼klendi!"

# 3. Enter Prompt
"Python'da quicksort algoritmasÄ± yaz ve time complexity'sini aÃ§Ä±kla"

# 4. Set Parameters
Max Length: 512
Temperature: 0.7
Top-p: 0.9

# 5. Generate
Click "ğŸš€ Ãœret"

# 6. Get Result
def quicksort(arr):
    if len(arr) <= 1:
        return arr
    
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    
    return quicksort(left) + middle + quicksort(right)

# Time Complexity: O(n log n) average case, O(nÂ²) worst case...
```

### 6. Troubleshooting

#### Common Issues & Solutions

**"âŒ Model yÃ¼klenirken hata"**
- Check internet connection
- Ensure sufficient disk space (50GB+)
- Try different model

**"âŒ LÃ¼tfen Ã¶nce bir model seÃ§in"**
- Load a model first using "ğŸ“¥ Model YÃ¼kle"
- Wait for confirmation message

**Slow Generation**
- Use GPU-enabled environment (Colab)
- Reduce max_length parameter
- Clear cache if memory full

**Poor Quality Output**
- Try different temperature values
- Improve prompt specificity
- Test with different models

### 7. System Requirements

**Minimum:**
- Python 3.8+
- 8GB RAM
- 50GB storage

**Recommended:**
- CUDA GPU (RTX 3060+)
- 16GB+ RAM
- 100GB+ SSD storage
- Stable internet connection

### 8. Support

**Quick Help:**
- Click examples below the prompt box
- Use "ğŸ“¦ Cache Durumu" to monitor system
- Try "ğŸ§¹ TÃ¼mÃ¼nÃ¼ Temizle" if issues persist

**Performance Tips:**
- Keep max 3 models cached
- Use appropriate parameters for your task
- Monitor GPU memory usage
- Restart if experiencing memory issues

---

**ğŸ¯ Ready to start? Select a model, enter your prompt, and experience AI-powered code generation!**
