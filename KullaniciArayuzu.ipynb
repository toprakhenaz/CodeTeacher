{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5c7df6FXejWq"
      },
      "outputs": [],
      "source": [
        "!pip install gradio transformers torch accelerate bitsandbytes\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import gc\n",
        "\n",
        "# Model listesi - Hem Toprak Henaz hem de ByGedik modelleri\n",
        "MODELS = {\n",
        "    # Toprak Henaz Modelleri\n",
        "    \"DeepSeek Coder 6.7B (ToprakH)\": \"toprakhenaz/DeepSeek-Coder-6.7B-Instruct\",\n",
        "    \"CodeLlama Magicoder 7B (ToprakH)\": \"toprakhenaz/codellama_magicoder-7b-lora-adapters\",\n",
        "    \"Gemma 7B (ToprakH)\": \"toprakhenaz/gemma-7b-lora-adapters\",\n",
        "    \"Qwen2.5 Coder 7B (ToprakH)\": \"toprakhenaz/qwen2.5-coder-7b-lora-adapters\",\n",
        "\n",
        "    # ByGedik Modelleri\n",
        "    \"Llama 3.1 8B CodeAlpaca (ByGedik)\": \"ByGedik/Meta-Llama-3.1-8B-Instruct-bnb-4bit-codeAlpaca-CodePlusExplanation\",\n",
        "    \"Mistral CodeAlpaca V3 (ByGedik)\": \"ByGedik/mistral-codealpaca-v3epoch\"\n",
        "}\n",
        "\n",
        "# Global deƒüi≈ükenler\n",
        "current_model = None\n",
        "current_tokenizer = None\n",
        "current_model_name = None\n",
        "\n",
        "# Model cache sistemi\n",
        "model_cache = {}  # {model_name: {\"model\": model, \"tokenizer\": tokenizer, \"last_used\": timestamp}}\n",
        "max_cached_models = 3  # Maksimum cache'lenecek model sayƒ±sƒ±\n",
        "import time\n",
        "\n",
        "# 4-bit quantization config (GPU bellek kullanƒ±mƒ±nƒ± azaltmak i√ßin)\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "def load_model(model_name):\n",
        "    \"\"\"Se√ßilen modeli y√ºkle veya cache'den al\"\"\"\n",
        "    global current_model, current_tokenizer, current_model_name, model_cache\n",
        "\n",
        "    if current_model_name == model_name:\n",
        "        return f\"‚úÖ {model_name} zaten aktif!\"\n",
        "\n",
        "    # Cache'de var mƒ± kontrol et\n",
        "    if model_name in model_cache:\n",
        "        # Cache'den al\n",
        "        current_model = model_cache[model_name][\"model\"]\n",
        "        current_tokenizer = model_cache[model_name][\"tokenizer\"]\n",
        "        current_model_name = model_name\n",
        "        model_cache[model_name][\"last_used\"] = time.time()\n",
        "\n",
        "        cache_info = get_cache_info()\n",
        "        return f\"üöÄ {model_name} cache'den hƒ±zlƒ±ca y√ºklendi!\\n{cache_info}\"\n",
        "\n",
        "    try:\n",
        "        model_path = MODELS[model_name]\n",
        "\n",
        "        # Cache boyut kontrol√º - gerekirse eski modeli sil\n",
        "        if len(model_cache) >= max_cached_models:\n",
        "            remove_oldest_from_cache()\n",
        "\n",
        "        # Tokenizer'ƒ± y√ºkle\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        # Modeli y√ºkle\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            quantization_config=quantization_config,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Cache'e ekle\n",
        "        model_cache[model_name] = {\n",
        "            \"model\": model,\n",
        "            \"tokenizer\": tokenizer,\n",
        "            \"last_used\": time.time()\n",
        "        }\n",
        "\n",
        "        # Aktif model olarak ayarla\n",
        "        current_model = model\n",
        "        current_tokenizer = tokenizer\n",
        "        current_model_name = model_name\n",
        "\n",
        "        cache_info = get_cache_info()\n",
        "        return f\"‚úÖ {model_name} ba≈üarƒ±yla y√ºklendi ve cache'lendi!\\n{cache_info}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Model y√ºklenirken hata: {str(e)}\"\n",
        "\n",
        "def remove_oldest_from_cache():\n",
        "    \"\"\"En eski kullanƒ±lan modeli cache'den sil\"\"\"\n",
        "    global model_cache\n",
        "\n",
        "    if not model_cache:\n",
        "        return\n",
        "\n",
        "    # En eski modeli bul\n",
        "    oldest_model = min(model_cache.keys(),\n",
        "                      key=lambda x: model_cache[x][\"last_used\"])\n",
        "\n",
        "    # Cache'den sil\n",
        "    del model_cache[oldest_model][\"model\"]\n",
        "    del model_cache[oldest_model][\"tokenizer\"]\n",
        "    del model_cache[oldest_model]\n",
        "\n",
        "    # GPU belleƒüini temizle\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def get_cache_info():\n",
        "    \"\"\"Cache durumunu g√∂ster\"\"\"\n",
        "    global model_cache\n",
        "\n",
        "    if not model_cache:\n",
        "        return \"üì¶ Cache bo≈ü\"\n",
        "\n",
        "    cache_list = []\n",
        "    for name in model_cache.keys():\n",
        "        last_used = model_cache[name][\"last_used\"]\n",
        "        time_diff = int(time.time() - last_used)\n",
        "        cache_list.append(f\"‚Ä¢ {name} ({time_diff}s √∂nce)\")\n",
        "\n",
        "    return f\"üì¶ Cache'deki modeller ({len(model_cache)}/{max_cached_models}):\\n\" + \"\\n\".join(cache_list)\n",
        "\n",
        "def generate_response(prompt, max_length=512, temperature=0.7, top_p=0.9):\n",
        "    \"\"\"Se√ßili model ile metin √ºret\"\"\"\n",
        "    global current_model, current_tokenizer\n",
        "\n",
        "    if current_model is None:\n",
        "        return \"‚ùå L√ºtfen √∂nce bir model se√ßin ve y√ºkleyin!\"\n",
        "\n",
        "    try:\n",
        "        # Prompt'u encode et\n",
        "        inputs = current_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = inputs.cuda()\n",
        "\n",
        "        # Metin √ºret\n",
        "        with torch.no_grad():\n",
        "            outputs = current_model.generate(\n",
        "                inputs,\n",
        "                max_length=max_length,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                do_sample=True,\n",
        "                pad_token_id=current_tokenizer.eos_token_id,\n",
        "                eos_token_id=current_tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode et ve prompt'u √ßƒ±kar\n",
        "        generated_text = current_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        response = generated_text[len(prompt):].strip()\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Metin √ºretilirken hata: {str(e)}\"\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"T√ºm cache'i ve belleƒüi temizle\"\"\"\n",
        "    global current_model, current_tokenizer, current_model_name, model_cache\n",
        "\n",
        "    # T√ºm cache'lenmi≈ü modelleri temizle\n",
        "    for cached_model in model_cache.values():\n",
        "        del cached_model[\"model\"]\n",
        "        del cached_model[\"tokenizer\"]\n",
        "\n",
        "    model_cache.clear()\n",
        "    current_model = None\n",
        "    current_tokenizer = None\n",
        "    current_model_name = None\n",
        "\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return \"üßπ T√ºm cache ve bellek temizlendi!\"\n",
        "\n",
        "def clear_single_model(model_name):\n",
        "    \"\"\"Tek bir modeli cache'den sil\"\"\"\n",
        "    global model_cache, current_model, current_tokenizer, current_model_name\n",
        "\n",
        "    if model_name not in model_cache:\n",
        "        return f\"‚ùå {model_name} cache'de bulunamadƒ±!\"\n",
        "\n",
        "    # Aktif model ise sƒ±fƒ±rla\n",
        "    if current_model_name == model_name:\n",
        "        current_model = None\n",
        "        current_tokenizer = None\n",
        "        current_model_name = None\n",
        "\n",
        "    # Cache'den sil\n",
        "    del model_cache[model_name][\"model\"]\n",
        "    del model_cache[model_name][\"tokenizer\"]\n",
        "    del model_cache[model_name]\n",
        "\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    cache_info = get_cache_info()\n",
        "    return f\"üóëÔ∏è {model_name} cache'den silindi!\\n{cache_info}\"\n",
        "\n",
        "# Gradio aray√ºz√º\n",
        "def create_interface():\n",
        "    with gr.Blocks(title=\"Model Switcher\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"# ü§ñ Multi-Profile Model Switcher\")\n",
        "        gr.Markdown(\"Toprak Henaz ve ByGedik profillerindeki modelleri se√ßin ve kullanƒ±n!\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                model_dropdown = gr.Dropdown(\n",
        "                    choices=list(MODELS.keys()),\n",
        "                    label=\"üéØ Model Se√ßin\",\n",
        "                    value=list(MODELS.keys())[0]\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    load_btn = gr.Button(\"üì• Model Y√ºkle\", variant=\"primary\")\n",
        "                    clear_btn = gr.Button(\"üßπ T√ºm√ºn√º Temizle\", variant=\"secondary\")\n",
        "\n",
        "                # Cache y√∂netimi\n",
        "                with gr.Row():\n",
        "                    cache_info_btn = gr.Button(\"üì¶ Cache Durumu\", variant=\"secondary\")\n",
        "                    clear_single_btn = gr.Button(\"üóëÔ∏è Bu Modeli Sil\", variant=\"secondary\")\n",
        "\n",
        "                status_text = gr.Textbox(\n",
        "                    label=\"üìä Durum & Cache Bilgisi\",\n",
        "                    value=\"Model se√ßin ve y√ºkleyin...\",\n",
        "                    interactive=False,\n",
        "                    lines=4\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                prompt_input = gr.Textbox(\n",
        "                    label=\"‚úçÔ∏è Prompt Girin\",\n",
        "                    placeholder=\"Kodlama sorunuz veya metin prompt'ƒ±nƒ±zƒ± buraya yazƒ±n...\",\n",
        "                    lines=3\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    max_length_slider = gr.Slider(\n",
        "                        minimum=50,\n",
        "                        maximum=1024,\n",
        "                        value=512,\n",
        "                        step=50,\n",
        "                        label=\"üìè Maksimum Uzunluk\"\n",
        "                    )\n",
        "\n",
        "                    temperature_slider = gr.Slider(\n",
        "                        minimum=0.1,\n",
        "                        maximum=2.0,\n",
        "                        value=0.7,\n",
        "                        step=0.1,\n",
        "                        label=\"üå°Ô∏è Temperature\"\n",
        "                    )\n",
        "\n",
        "                    top_p_slider = gr.Slider(\n",
        "                        minimum=0.1,\n",
        "                        maximum=1.0,\n",
        "                        value=0.9,\n",
        "                        step=0.1,\n",
        "                        label=\"üéØ Top-p\"\n",
        "                    )\n",
        "\n",
        "                generate_btn = gr.Button(\"üöÄ √úret\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "                output_text = gr.Textbox(\n",
        "                    label=\"üìù √áƒ±ktƒ±\",\n",
        "                    lines=10,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "        # Event handlers\n",
        "        load_btn.click(\n",
        "            fn=load_model,\n",
        "            inputs=[model_dropdown],\n",
        "            outputs=[status_text]\n",
        "        )\n",
        "\n",
        "        clear_btn.click(\n",
        "            fn=clear_memory,\n",
        "            outputs=[status_text]\n",
        "        )\n",
        "\n",
        "        cache_info_btn.click(\n",
        "            fn=get_cache_info,\n",
        "            outputs=[status_text]\n",
        "        )\n",
        "\n",
        "        clear_single_btn.click(\n",
        "            fn=clear_single_model,\n",
        "            inputs=[model_dropdown],\n",
        "            outputs=[status_text]\n",
        "        )\n",
        "\n",
        "        generate_btn.click(\n",
        "            fn=generate_response,\n",
        "            inputs=[prompt_input, max_length_slider, temperature_slider, top_p_slider],\n",
        "            outputs=[output_text]\n",
        "        )\n",
        "\n",
        "        # √ñrnekler\n",
        "        gr.Examples(\n",
        "            examples=[\n",
        "                [\"Python'da bir fibonacci fonksiyonu yaz ve a√ßƒ±kla\"],\n",
        "                [\"JavaScript ile REST API √ßaƒürƒ±sƒ± nasƒ±l yapƒ±lƒ±r?\"],\n",
        "                [\"SQL ile JOIN i≈ülemleri √∂rneƒüi\"],\n",
        "                [\"React functional component ile state management\"],\n",
        "                [\"Machine Learning model evaluation metrikleri\"],\n",
        "                [\"Docker container olu≈üturma adƒ±mlarƒ±\"],\n",
        "            ],\n",
        "            inputs=[prompt_input]\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        ### üéØ **Mevcut Modeller (6 adet)**:\n",
        "\n",
        "        **Toprak Henaz Profili:**\n",
        "        - DeepSeek Coder 6.7B (Kod √ºretimi)\n",
        "        - CodeLlama Magicoder 7B (Kod tamamlama)\n",
        "        - Gemma 7B (Genel ama√ßlƒ±)\n",
        "        - Qwen2.5 Coder 7B (Kod analizi)\n",
        "\n",
        "        **ByGedik Profili:**\n",
        "        - Llama 3.1 8B CodeAlpaca (Kod a√ßƒ±klamasƒ±)\n",
        "        - Mistral CodeAlpaca V3 (Kod √∂rnekleri)\n",
        "\n",
        "        ### üìã Kullanƒ±m Talimatlarƒ±:\n",
        "        1. **Model Se√ßin**: Dropdown'dan kullanmak istediƒüiniz modeli se√ßin\n",
        "        2. **Model Y√ºkle**: \"Model Y√ºkle\" butonuna tƒ±klayarak modeli cache'e y√ºkleyin\n",
        "        3. **Hƒ±zlƒ± Ge√ßi≈ü**: Cache'lenmi≈ü modeller arasƒ± anƒ±nda ge√ßi≈ü yapƒ±n\n",
        "        4. **Prompt Girin**: Kodlama sorunuzu veya metin prompt'ƒ±nƒ±zƒ± yazƒ±n\n",
        "        5. **Parametreleri Ayarlayƒ±n**: Uzunluk, temperature ve top-p deƒüerlerini ayarlayƒ±n\n",
        "        6. **√úret**: \"√úret\" butonuna tƒ±klayarak cevap alƒ±n\n",
        "\n",
        "        ### üöÄ Cache Sistemi:\n",
        "        - **Maksimum 3 model** aynƒ± anda cache'de tutulur\n",
        "        - **LRU (Least Recently Used)** algoritmasƒ± ile eski modeller otomatik temizlenir\n",
        "        - **Anƒ±nda ge√ßi≈ü** cache'lenmi≈ü modeller arasƒ±nda\n",
        "        - **Bellek y√∂netimi** otomatik optimizasyon\n",
        "\n",
        "        ### ‚öôÔ∏è Parametre A√ßƒ±klamalarƒ±:\n",
        "        - **Maksimum Uzunluk**: √úretilecek metnin maksimum token sayƒ±sƒ±\n",
        "        - **Temperature**: Yaratƒ±cƒ±lƒ±k seviyesi (d√º≈ü√ºk = daha tutarlƒ±, y√ºksek = daha yaratƒ±cƒ±)\n",
        "        - **Top-p**: Kelime se√ßimi √ße≈üitliliƒüi (0.9 √∂nerilir)\n",
        "\n",
        "        ### üí° ƒ∞pu√ßlarƒ±:\n",
        "        - **Cache sistemi** sayesinde modeller arasƒ± hƒ±zlƒ± ge√ßi≈ü yapabilirsiniz\n",
        "        - **3 model** aynƒ± anda cache'de tutulur, daha fazlasƒ± i√ßin otomatik temizlik\n",
        "        - **GPU bellek optimizasyonu** 4-bit quantization ile saƒülanƒ±r\n",
        "        - **Cache durumunu** \"Cache Durumu\" butonu ile kontrol edebilirsiniz\n",
        "        - **ToprakH modelleri** genel kod √ºretimi i√ßin optimize edilmi≈ü\n",
        "        - **ByGedik modelleri** CodeAlpaca dataset'i ile fine-tune edilmi≈ü\n",
        "        - **Model kar≈üƒ±la≈ütƒ±rmasƒ±** i√ßin aynƒ± prompt'u farklƒ± modellerde test edin\n",
        "        - **Bellek temizliƒüi** gerektiƒüinde \"T√ºm√ºn√º Temizle\" veya \"Bu Modeli Sil\" kullanƒ±n\n",
        "        \"\"\")\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Uygulamayƒ± ba≈ülat\n",
        "if __name__ == \"__main__\":\n",
        "    demo = create_interface()\n",
        "    demo.launch(\n",
        "        share=True,  # Public link olu≈ütur\n",
        "        debug=True,\n",
        "        height=800\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}