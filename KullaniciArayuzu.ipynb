{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5c7df6FXejWq"
      },
      "outputs": [],
      "source": [
        "!pip install gradio transformers torch accelerate bitsandbytes\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import gc\n",
        "\n",
        "# Model listesi - Hem Toprak Henaz hem de ByGedik modelleri\n",
        "MODELS = {\n",
        "    # Toprak Henaz Modelleri\n",
        "    \"DeepSeek Coder 6.7B (ToprakH)\": \"toprakhenaz/DeepSeek-Coder-6.7B-Instruct\",\n",
        "    \"CodeLlama Magicoder 7B (ToprakH)\": \"toprakhenaz/codellama_magicoder-7b-lora-adapters\",\n",
        "    \"Gemma 7B (ToprakH)\": \"toprakhenaz/gemma-7b-lora-adapters\",\n",
        "    \"Qwen2.5 Coder 7B (ToprakH)\": \"toprakhenaz/qwen2.5-coder-7b-lora-adapters\",\n",
        "\n",
        "    # ByGedik Modelleri\n",
        "    \"Llama 3.1 8B CodeAlpaca (ByGedik)\": \"ByGedik/Meta-Llama-3.1-8B-Instruct-bnb-4bit-codeAlpaca-CodePlusExplanation\",\n",
        "    \"Mistral CodeAlpaca V3 (ByGedik)\": \"ByGedik/mistral-codealpaca-v3epoch\"\n",
        "}\n",
        "\n",
        "# Global deÄŸiÅŸkenler\n",
        "current_model = None\n",
        "current_tokenizer = None\n",
        "current_model_name = None\n",
        "\n",
        "# Model cache sistemi\n",
        "model_cache = {}  # {model_name: {\"model\": model, \"tokenizer\": tokenizer, \"last_used\": timestamp}}\n",
        "max_cached_models = 3  # Maksimum cache'lenecek model sayÄ±sÄ±\n",
        "import time\n",
        "\n",
        "# 4-bit quantization config (GPU bellek kullanÄ±mÄ±nÄ± azaltmak iÃ§in)\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "def load_model(model_name):\n",
        "    \"\"\"SeÃ§ilen modeli yÃ¼kle veya cache'den al\"\"\"\n",
        "    global current_model, current_tokenizer, current_model_name, model_cache\n",
        "\n",
        "    if current_model_name == model_name:\n",
        "        return f\"âœ… {model_name} zaten aktif!\"\n",
        "\n",
        "    # Cache'de var mÄ± kontrol et\n",
        "    if model_name in model_cache:\n",
        "        # Cache'den al\n",
        "        current_model = model_cache[model_name][\"model\"]\n",
        "        current_tokenizer = model_cache[model_name][\"tokenizer\"]\n",
        "        current_model_name = model_name\n",
        "        model_cache[model_name][\"last_used\"] = time.time()\n",
        "\n",
        "        cache_info = get_cache_info()\n",
        "        return f\"ğŸš€ {model_name} cache'den hÄ±zlÄ±ca yÃ¼klendi!\\n{cache_info}\"\n",
        "\n",
        "    try:\n",
        "        model_path = MODELS[model_name]\n",
        "\n",
        "        # Cache boyut kontrolÃ¼ - gerekirse eski modeli sil\n",
        "        if len(model_cache) >= max_cached_models:\n",
        "            remove_oldest_from_cache()\n",
        "\n",
        "        # Tokenizer'Ä± yÃ¼kle\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        # Modeli yÃ¼kle\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            quantization_config=quantization_config,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Cache'e ekle\n",
        "        model_cache[model_name] = {\n",
        "            \"model\": model,\n",
        "            \"tokenizer\": tokenizer,\n",
        "            \"last_used\": time.time()\n",
        "        }\n",
        "\n",
        "        # Aktif model olarak ayarla\n",
        "        current_model = model\n",
        "        current_tokenizer = tokenizer\n",
        "        current_model_name = model_name\n",
        "\n",
        "        cache_info = get_cache_info()\n",
        "        return f\"âœ… {model_name} baÅŸarÄ±yla yÃ¼klendi ve cache'lendi!\\n{cache_info}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Model yÃ¼klenirken hata: {str(e)}\"\n",
        "\n",
        "def remove_oldest_from_cache():\n",
        "    \"\"\"En eski kullanÄ±lan modeli cache'den sil\"\"\"\n",
        "    global model_cache\n",
        "\n",
        "    if not model_cache:\n",
        "        return\n",
        "\n",
        "    # En eski modeli bul\n",
        "    oldest_model = min(model_cache.keys(),\n",
        "                      key=lambda x: model_cache[x][\"last_used\"])\n",
        "\n",
        "    # Cache'den sil\n",
        "    del model_cache[oldest_model][\"model\"]\n",
        "    del model_cache[oldest_model][\"tokenizer\"]\n",
        "    del model_cache[oldest_model]\n",
        "\n",
        "    # GPU belleÄŸini temizle\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def get_cache_info():\n",
        "    \"\"\"Cache durumunu gÃ¶ster\"\"\"\n",
        "    global model_cache\n",
        "\n",
        "    if not model_cache:\n",
        "        return \"ğŸ“¦ Cache boÅŸ\"\n",
        "\n",
        "    cache_list = []\n",
        "    for name in model_cache.keys():\n",
        "        last_used = model_cache[name][\"last_used\"]\n",
        "        time_diff = int(time.time() - last_used)\n",
        "        cache_list.append(f\"â€¢ {name} ({time_diff}s Ã¶nce)\")\n",
        "\n",
        "    return f\"ğŸ“¦ Cache'deki modeller ({len(model_cache)}/{max_cached_models}):\\n\" + \"\\n\".join(cache_list)\n",
        "\n",
        "def generate_response(prompt, max_length=512, temperature=0.7, top_p=0.9):\n",
        "    \"\"\"SeÃ§ili model ile metin Ã¼ret\"\"\"\n",
        "    global current_model, current_tokenizer\n",
        "\n",
        "    if current_model is None:\n",
        "        return \"âŒ LÃ¼tfen Ã¶nce bir model seÃ§in ve yÃ¼kleyin!\"\n",
        "\n",
        "    try:\n",
        "        # Prompt'u encode et\n",
        "        inputs = current_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = inputs.cuda()\n",
        "\n",
        "        # Metin Ã¼ret\n",
        "        with torch.no_grad():\n",
        "            outputs = current_model.generate(\n",
        "                inputs,\n",
        "                max_length=max_length,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                do_sample=True,\n",
        "                pad_token_id=current_tokenizer.eos_token_id,\n",
        "                eos_token_id=current_tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode et ve prompt'u Ã§Ä±kar\n",
        "        generated_text = current_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        response = generated_text[len(prompt):].strip()\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Metin Ã¼retilirken hata: {str(e)}\"\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"TÃ¼m cache'i ve belleÄŸi temizle\"\"\"\n",
        "    global current_model, current_tokenizer, current_model_name, model_cache\n",
        "\n",
        "    # TÃ¼m cache'lenmiÅŸ modelleri temizle\n",
        "    for cached_model in model_cache.values():\n",
        "        del cached_model[\"model\"]\n",
        "        del cached_model[\"tokenizer\"]\n",
        "\n",
        "    model_cache.clear()\n",
        "    current_model = None\n",
        "    current_tokenizer = None\n",
        "    current_model_name = None\n",
        "\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return \"ğŸ§¹ TÃ¼m cache ve bellek temizlendi!\"\n",
        "\n",
        "def clear_single_model(model_name):\n",
        "    \"\"\"Tek bir modeli cache'den sil\"\"\"\n",
        "    global model_cache, current_model, current_tokenizer, current_model_name\n",
        "\n",
        "    if model_name not in model_cache:\n",
        "        return f\"âŒ {model_name} cache'de bulunamadÄ±!\"\n",
        "\n",
        "    # Aktif model ise sÄ±fÄ±rla\n",
        "    if current_model_name == model_name:\n",
        "        current_model = None\n",
        "        current_tokenizer = None\n",
        "        current_model_name = None\n",
        "\n",
        "    # Cache'den sil\n",
        "    del model_cache[model_name][\"model\"]\n",
        "    del model_cache[model_name][\"tokenizer\"]\n",
        "    del model_cache[model_name]\n",
        "\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    cache_info = get_cache_info()\n",
        "    return f\"ğŸ—‘ï¸ {model_name} cache'den silindi!\\n{cache_info}\"\n",
        "\n",
        "# Gradio arayÃ¼zÃ¼\n",
        "def create_interface():\n",
        "    with gr.Blocks(title=\"Model Switcher\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"# ğŸ¤– Multi-Profile Model Switcher\")\n",
        "        gr.Markdown(\"Toprak Henaz ve ByGedik profillerindeki modelleri seÃ§in ve kullanÄ±n!\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                model_dropdown = gr.Dropdown(\n",
        "                    choices=list(MODELS.keys()),\n",
        "                    label=\"ğŸ¯ Model SeÃ§in\",\n",
        "                    value=list(MODELS.keys())[0]\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    load_btn = gr.Button(\"ğŸ“¥ Model YÃ¼kle\", variant=\"primary\")\n",
        "                    clear_btn = gr.Button(\"ğŸ§¹ TÃ¼mÃ¼nÃ¼ Temizle\", variant=\"secondary\")\n",
        "\n",
        "                # Cache yÃ¶netimi\n",
        "                with gr.Row():\n",
        "                    cache_info_btn = gr.Button(\"ğŸ“¦ Cache Durumu\", variant=\"secondary\")\n",
        "                    clear_single_btn = gr.Button(\"ğŸ—‘ï¸ Bu Modeli Sil\", variant=\"secondary\")\n",
        "\n",
        "                status_text = gr.Textbox(\n",
        "                    label=\"ğŸ“Š Durum & Cache Bilgisi\",\n",
        "                    value=\"Model seÃ§in ve yÃ¼kleyin...\",\n",
        "                    interactive=False,\n",
        "                    lines=4\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                prompt_input = gr.Textbox(\n",
        "                    label=\"âœï¸ Prompt Girin\",\n",
        "                    placeholder=\"Kodlama sorunuz veya metin prompt'Ä±nÄ±zÄ± buraya yazÄ±n...\",\n",
        "                    lines=3\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    max_length_slider = gr.Slider(\n",
        "                        minimum=50,\n",
        "                        maximum=1024,\n",
        "                        value=512,\n",
        "                        step=50,\n",
        "                        label=\"ğŸ“ Maksimum Uzunluk\"\n",
        "                    )\n",
        "\n",
        "                    temperature_slider = gr.Slider(\n",
        "                        minimum=0.1,\n",
        "                        maximum=2.0,\n",
        "                        value=0.7,\n",
        "                        step=0.1,\n",
        "                        label=\"ğŸŒ¡ï¸ Temperature\"\n",
        "                    )\n",
        "\n",
        "                    top_p_slider = gr.Slider(\n",
        "                        minimum=0.1,\n",
        "                        maximum=1.0,\n",
        "                        value=0.9,\n",
        "                        step=0.1,\n",
        "                        label=\"ğŸ¯ Top-p\"\n",
        "                    )\n",
        "\n",
        "                generate_btn = gr.Button(\"ğŸš€ Ãœret\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "                output_text = gr.Textbox(\n",
        "                    label=\"ğŸ“ Ã‡Ä±ktÄ±\",\n",
        "                    lines=10,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "        # Event handlers\n",
        "        load_btn.click(\n",
        "            fn=load_model,\n",
        "            inputs=[model_dropdown],\n",
        "            outputs=[status_text]\n",
        "        )\n",
        "\n",
        "        clear_btn.click(\n",
        "            fn=clear_memory,\n",
        "            outputs=[status_text]\n",
        "        )\n",
        "\n",
        "        cache_info_btn.click(\n",
        "            fn=get_cache_info,\n",
        "            outputs=[status_text]\n",
        "        )\n",
        "\n",
        "        clear_single_btn.click(\n",
        "            fn=clear_single_model,\n",
        "            inputs=[model_dropdown],\n",
        "            outputs=[status_text]\n",
        "        )\n",
        "\n",
        "        generate_btn.click(\n",
        "            fn=generate_response,\n",
        "            inputs=[prompt_input, max_length_slider, temperature_slider, top_p_slider],\n",
        "            outputs=[output_text]\n",
        "        )\n",
        "\n",
        "        # Ã–rnekler\n",
        "        gr.Examples(\n",
        "            examples=[\n",
        "                [\"Python'da bir fibonacci fonksiyonu yaz ve aÃ§Ä±kla\"],\n",
        "                [\"JavaScript ile REST API Ã§aÄŸrÄ±sÄ± nasÄ±l yapÄ±lÄ±r?\"],\n",
        "                [\"SQL ile JOIN iÅŸlemleri Ã¶rneÄŸi\"],\n",
        "                [\"React functional component ile state management\"],\n",
        "                [\"Machine Learning model evaluation metrikleri\"],\n",
        "                [\"Docker container oluÅŸturma adÄ±mlarÄ±\"],\n",
        "            ],\n",
        "            inputs=[prompt_input]\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        ### ğŸ¯ **Mevcut Modeller (6 adet)**:\n",
        "\n",
        "        **Toprak Henaz Profili:**\n",
        "        - DeepSeek Coder 6.7B (Kod Ã¼retimi)\n",
        "        - CodeLlama Magicoder 7B (Kod tamamlama)\n",
        "        - Gemma 7B (Genel amaÃ§lÄ±)\n",
        "        - Qwen2.5 Coder 7B (Kod analizi)\n",
        "\n",
        "        **ByGedik Profili:**\n",
        "        - Llama 3.1 8B CodeAlpaca (Kod aÃ§Ä±klamasÄ±)\n",
        "        - Mistral CodeAlpaca V3 (Kod Ã¶rnekleri)\n",
        "\n",
        "        ### ğŸ“‹ KullanÄ±m TalimatlarÄ±:\n",
        "        1. **Model SeÃ§in**: Dropdown'dan kullanmak istediÄŸiniz modeli seÃ§in\n",
        "        2. **Model YÃ¼kle**: \"Model YÃ¼kle\" butonuna tÄ±klayarak modeli cache'e yÃ¼kleyin\n",
        "        3. **HÄ±zlÄ± GeÃ§iÅŸ**: Cache'lenmiÅŸ modeller arasÄ± anÄ±nda geÃ§iÅŸ yapÄ±n\n",
        "        4. **Prompt Girin**: Kodlama sorunuzu veya metin prompt'Ä±nÄ±zÄ± yazÄ±n\n",
        "        5. **Parametreleri AyarlayÄ±n**: Uzunluk, temperature ve top-p deÄŸerlerini ayarlayÄ±n\n",
        "        6. **Ãœret**: \"Ãœret\" butonuna tÄ±klayarak cevap alÄ±n\n",
        "\n",
        "        ### ğŸš€ Cache Sistemi:\n",
        "        - **Maksimum 3 model** aynÄ± anda cache'de tutulur\n",
        "        - **LRU (Least Recently Used)** algoritmasÄ± ile eski modeller otomatik temizlenir\n",
        "        - **AnÄ±nda geÃ§iÅŸ** cache'lenmiÅŸ modeller arasÄ±nda\n",
        "        - **Bellek yÃ¶netimi** otomatik optimizasyon\n",
        "\n",
        "        ### âš™ï¸ Parametre AÃ§Ä±klamalarÄ±:\n",
        "        - **Maksimum Uzunluk**: Ãœretilecek metnin maksimum token sayÄ±sÄ±\n",
        "        - **Temperature**: YaratÄ±cÄ±lÄ±k seviyesi (dÃ¼ÅŸÃ¼k = daha tutarlÄ±, yÃ¼ksek = daha yaratÄ±cÄ±)\n",
        "        - **Top-p**: Kelime seÃ§imi Ã§eÅŸitliliÄŸi (0.9 Ã¶nerilir)\n",
        "\n",
        "        ### ğŸ’¡ Ä°puÃ§larÄ±:\n",
        "        - **Cache sistemi** sayesinde modeller arasÄ± hÄ±zlÄ± geÃ§iÅŸ yapabilirsiniz\n",
        "        - **3 model** aynÄ± anda cache'de tutulur, daha fazlasÄ± iÃ§in otomatik temizlik\n",
        "        - **GPU bellek optimizasyonu** 4-bit quantization ile saÄŸlanÄ±r\n",
        "        - **Cache durumunu** \"Cache Durumu\" butonu ile kontrol edebilirsiniz\n",
        "        - **ToprakH modelleri** genel kod Ã¼retimi iÃ§in optimize edilmiÅŸ\n",
        "        - **ByGedik modelleri** CodeAlpaca dataset'i ile fine-tune edilmiÅŸ\n",
        "        - **Model karÅŸÄ±laÅŸtÄ±rmasÄ±** iÃ§in aynÄ± prompt'u farklÄ± modellerde test edin\n",
        "        - **Bellek temizliÄŸi** gerektiÄŸinde \"TÃ¼mÃ¼nÃ¼ Temizle\" veya \"Bu Modeli Sil\" kullanÄ±n\n",
        "        \"\"\")\n",
        "\n",
        "    return demo\n",
        "\n",
        "# UygulamayÄ± baÅŸlat\n",
        "if __name__ == \"__main__\":\n",
        "    demo = create_interface()\n",
        "    demo.launch(\n",
        "        share=True,  # Public link oluÅŸtur\n",
        "        debug=True,\n",
        "        height=800\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}